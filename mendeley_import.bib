@article{al.2017Attention,
  title = {Attention Is All You Need},
  author = {Vaswani et al.},
  year = {2017},
  journal = {NeurIPS},
  doi = {10.1234/transformer},
  url = {https://arxiv.org/abs/1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks.},
}


@article{al.2019BERT:,
  title = {BERT: Pre-training of Deep Bidirectional Transformers},
  author = {Devlin et al.},
  year = {2019},
  journal = {NAACL},
  doi = {10.1234/bert},
  url = {https://arxiv.org/abs/1810.04805},
  abstract = {We introduce BERT, a new language representation model.},
}


@article{al.2020GPT-3:,
  title = {GPT-3: Language Models are Few-Shot Learners},
  author = {Brown et al.},
  year = {2020},
  journal = {NeurIPS},
  doi = {10.1234/gpt3},
  url = {https://arxiv.org/abs/2005.14165},
  abstract = {Recent work has demonstrated substantial gains on NLP tasks.},
}
