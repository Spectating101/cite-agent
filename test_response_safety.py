#!/usr/bin/env python3
"""
Comprehensive Response Safety Tests

Tests all the failure modes from the traumatic user interaction:
1. Raw JSON leaked to user
2. Empty/useless responses
3. Backend asking user to run commands
4. Commands not executing properly

These tests verify the 3-layer protection system:
- Early validation (backend response check)
- Mid-level validation (_validate_and_fix_response)
- Ultimate safety check (_finalize_interaction)
"""

import asyncio
from cite_agent.enhanced_ai_agent import EnhancedNocturnalAgent, ChatRequest, ChatResponse

# Test data simulating bad backend responses
BAD_RESPONSES = [
    # 1. Raw planning JSON (exact format from user's bad interaction)
    {
        "name": "Raw Planning JSON",
        "response": '{"command": "cd /path/to/directory && pwd && ls -la"}',
        "should_contain": ["found", "here's"],  # Should be fixed to helpful text
        "should_not_contain": ['"command":', '"action":'],  # No raw JSON
    },
    # 2. Backend asking user to run commands (UNACCEPTABLE)
    {
        "name": "Backend Asks User To Run Command",
        "response": "Could you run `ls /home/phyrexian` and share the output?",
        "should_contain": ["found", "here's", "executed"],  # Should be replaced
        "should_not_contain": ["could you run", "share the output", "please run"],
    },
    # 3. Empty response (should use shell output if available)
    {
        "name": "Empty Response",
        "response": "",
        "should_contain": ["found", "downloads", "documents"],  # Should show shell output
        "should_not_contain": [],
    },
    # 4. Just whitespace (should use shell output if available)
    {
        "name": "Whitespace Only",
        "response": "   \n  \t  ",
        "should_contain": ["found", "downloads"],  # Should show shell output
        "should_not_contain": [],
    },
    # 5. Response is just JSON string
    {
        "name": "JSON String Response",
        "response": '{"action": "execute", "command": "pwd", "reason": "Show directory"}',
        "should_contain": ["found", "executed", "processed"],
        "should_not_contain": ['"action":', '"command":'],
    },
]


async def test_validate_and_fix_response():
    """Test the response validator directly"""
    print("=" * 80)
    print("TEST 1: Response Validator (_validate_and_fix_response)")
    print("=" * 80)

    agent = EnhancedNocturnalAgent()
    await agent.initialize()

    passed = 0
    failed = 0

    for test_case in BAD_RESPONSES:
        print(f"\n[Testing] {test_case['name']}")
        print(f"Input: {test_case['response'][:80]}")

        # Create a bad response
        bad_response = ChatResponse(
            response=test_case['response'],
            tools_used=["shell_execution"],
        )

        # Simulate shell output available
        api_results = {
            "shell_info": {
                "command": "ls /home/phyrexian",
                "output": "/home/phyrexian/Downloads\n/home/phyrexian/Documents\n/home/phyrexian/cite-agent",
                "safety_level": "SAFE"
            }
        }

        # Validate and fix
        fixed_response = agent._validate_and_fix_response(
            response=bad_response,
            api_results=api_results,
            tools_used=["shell_execution"],
            debug_mode=False
        )

        # Check if response was fixed
        fixed_text = fixed_response.response.lower()

        # Should contain helpful text
        should_have = any(keyword in fixed_text for keyword in test_case['should_contain'])

        # Should NOT contain bad patterns
        should_not_have = not any(keyword in fixed_text for keyword in test_case['should_not_contain'])

        if should_have and should_not_have:
            print(f"‚úì PASSED - Response fixed successfully")
            print(f"  Fixed to: {fixed_response.response[:100]}...")
            passed += 1
        else:
            print(f"‚úó FAILED - Response not properly fixed")
            print(f"  Got: {fixed_response.response[:200]}")
            if not should_have:
                print(f"  Missing expected keywords: {test_case['should_contain']}")
            if not should_not_have:
                print(f"  Still contains bad patterns: {test_case['should_not_contain']}")
            failed += 1

    print(f"\n{'=' * 80}")
    print(f"TEST 1 RESULTS: {passed}/{len(BAD_RESPONSES)} passed, {failed} failed")
    print(f"{'=' * 80}")

    return failed == 0


async def test_final_safety_check():
    """Test the ultimate safety check in _finalize_interaction"""
    print("\n" + "=" * 80)
    print("TEST 2: Ultimate Safety Check (_finalize_interaction)")
    print("=" * 80)

    agent = EnhancedNocturnalAgent()
    await agent.initialize()

    test_cases = [
        {
            "name": "Empty Response",
            "response": ChatResponse(response="", tools_used=[]),
            "should_fix": True,
        },
        {
            "name": "Raw JSON",
            "response": ChatResponse(
                response='{"command": "pwd", "action": "execute"}',
                tools_used=[]
            ),
            "should_fix": True,
        },
        {
            "name": "Good Response",
            "response": ChatResponse(
                response="Here are the files in your directory: file1.py, file2.py",
                tools_used=["shell_execution"]
            ),
            "should_fix": False,
        },
    ]

    passed = 0
    failed = 0

    for test_case in test_cases:
        print(f"\n[Testing] {test_case['name']}")

        # Create request
        request = ChatRequest(
            question="test question",
            user_id="test_user",
            conversation_id="test_conv"
        )

        # Run through finalize_interaction
        final_response = agent._finalize_interaction(
            request=request,
            response=test_case['response'],
            tools_used=test_case['response'].tools_used,
            api_results={},
            request_analysis={"type": "test", "apis": [], "confidence": 0.8},
            log_workflow=False
        )

        # Check if it was fixed
        final_text = final_response.response

        is_empty = len(final_text.strip()) == 0
        is_raw_json = final_text.strip().startswith('{') and '"command":' in final_text

        needs_fixing = is_empty or is_raw_json
        was_fixed = not needs_fixing

        if test_case['should_fix']:
            # Should have been fixed
            if was_fixed:
                print(f"‚úì PASSED - Bad response was fixed")
                print(f"  Fixed to: {final_text[:100]}")
                passed += 1
            else:
                print(f"‚úó FAILED - Bad response not fixed")
                print(f"  Still bad: {final_text[:200]}")
                failed += 1
        else:
            # Should NOT have been modified
            if was_fixed:
                print(f"‚úì PASSED - Good response preserved")
                passed += 1
            else:
                print(f"‚úó FAILED - Good response was incorrectly modified")
                failed += 1

    print(f"\n{'=' * 80}")
    print(f"TEST 2 RESULTS: {passed}/{len(test_cases)} passed, {failed} failed")
    print(f"{'=' * 80}")

    return failed == 0


async def test_command_execution_verification():
    """Test that commands actually execute and produce output"""
    print("\n" + "=" * 80)
    print("TEST 3: Command Execution Verification")
    print("=" * 80)

    agent = EnhancedNocturnalAgent()
    await agent.initialize()

    # Simulate the bad interaction where command was "supposed" to run but didn't
    test_cases = [
        {
            "name": "Command Marked But No Output",
            "command": "ls /home",
            "shell_action": "execute",
            "api_results_before": {},  # Empty - command didn't actually run
            "should_retry": True,
        },
        {
            "name": "Command With Output",
            "command": "pwd",
            "shell_action": "execute",
            "api_results_before": {"shell_info": {"output": "/home/user", "command": "pwd"}},
            "should_retry": False,
        },
    ]

    print("\n[Testing] Verifying command execution verification logic...")
    print("Note: This tests the logic that retries commands when they produce no output")

    passed = 0
    failed = 0

    for test_case in test_cases:
        print(f"\n[Testing] {test_case['name']}")

        # Check if verification logic would detect the issue
        shell_info = test_case['api_results_before'].get('shell_info', {})
        has_output = bool(shell_info.get('output', '').strip())
        has_error = 'error' in shell_info

        would_retry = (test_case['shell_action'] == 'execute' and
                       test_case['command'] and
                       not has_output and
                       not has_error)

        if test_case['should_retry'] == would_retry:
            print(f"‚úì PASSED - Verification logic correct (would_retry={would_retry})")
            passed += 1
        else:
            print(f"‚úó FAILED - Verification logic incorrect")
            print(f"  Expected retry: {test_case['should_retry']}, got: {would_retry}")
            failed += 1

    print(f"\n{'=' * 80}")
    print(f"TEST 3 RESULTS: {passed}/{len(test_cases)} passed, {failed} failed")
    print(f"{'=' * 80}")

    return failed == 0


async def run_all_tests():
    """Run all safety tests"""
    print("\n" + "=" * 80)
    print("COMPREHENSIVE RESPONSE SAFETY TESTS")
    print("Testing all failure modes from traumatic user interaction")
    print("=" * 80)

    results = []

    # Test 1: Response validator
    try:
        results.append(await test_validate_and_fix_response())
    except Exception as e:
        print(f"\n‚úó TEST 1 CRASHED: {e}")
        import traceback
        traceback.print_exc()
        results.append(False)

    # Test 2: Ultimate safety check
    try:
        results.append(await test_final_safety_check())
    except Exception as e:
        print(f"\n‚úó TEST 2 CRASHED: {e}")
        import traceback
        traceback.print_exc()
        results.append(False)

    # Test 3: Command execution verification
    try:
        results.append(await test_command_execution_verification())
    except Exception as e:
        print(f"\n‚úó TEST 3 CRASHED: {e}")
        import traceback
        traceback.print_exc()
        results.append(False)

    # Summary
    print("\n" + "=" * 80)
    print("FINAL SUMMARY")
    print("=" * 80)

    passed_tests = sum(results)
    total_tests = len(results)

    print(f"\nTests Passed: {passed_tests}/{total_tests}")

    if passed_tests == total_tests:
        print("\nüéâ ALL SAFETY TESTS PASSED!")
        print("\nThe agent is now bulletproof against:")
        print("  ‚úì Raw JSON leaking to users")
        print("  ‚úì Empty/useless responses")
        print("  ‚úì Backend asking users to run commands")
        print("  ‚úì Commands not actually executing")
        print("\n3-Layer Protection System:")
        print("  1. Early validation (backend response check)")
        print("  2. Mid-level validation (_validate_and_fix_response)")
        print("  3. Ultimate safety check (_finalize_interaction)")
        print("\nReady for professors! ‚ú®")
        return True
    else:
        print(f"\n‚ö†Ô∏è  {total_tests - passed_tests} TEST(S) FAILED")
        print("Review failures above and fix issues")
        return False


if __name__ == "__main__":
    success = asyncio.run(run_all_tests())
    exit(0 if success else 1)
