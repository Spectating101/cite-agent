# Pre-Ship Critical Path Testing Plan
**Version**: 1.5.7 Pre-Release Testing  
**Date**: November 20, 2024  
**Objective**: Test previously untested tool categories before Windows testing and PyPI ship

---

## Executive Summary

### Coverage Status (Before This Testing)
- **Total Tools**: 39
- **Tested**: 11 (28%)
- **Untested**: 28 (72%)

### Critical Gaps Identified
1. **Qualitative Analysis**: 0/5 tools tested (0%) - **UNIQUE DIFFERENTIATOR**
2. **Literature Synthesis**: 1/8 tools tested (12.5%) - **CORE RESEARCH FEATURE**
3. **Advanced Statistics**: 4/13 tools tested (31%)
4. **Web Search**: 0/1 tools tested (0%) - **FALLBACK MECHANISM**
5. **R Code Execution**: 0/1 tools tested (0%)

### Why This Matters
- **Qualitative analysis suite** is cite-agent's unique selling point (competitors don't offer this)
- **Literature synthesis tools** are critical for academic researchers (core use case)
- **Advanced stats** are differentiators for serious research work
- If these break, users lose the primary value propositions

---

## Test Plan Overview

### Phase 1: Qualitative Analysis Tools (15-20 min)
**Tools to test**: `load_transcript`, `create_code`, `code_segment`, `list_codes`, `extract_themes`

**Why critical**: This is the **only AI research assistant** that offers qualitative coding. If broken, cite-agent loses its unique competitive advantage.

### Phase 2: Literature Synthesis Tools (15-20 min)
**Tools to test**: `find_related_papers`, `add_paper`, `export_to_zotero`, `extract_lit_themes`, `find_research_gaps`, `synthesize_literature`, `export_lit_review`

**Why critical**: Core academic workflow. Researchers use cite-agent specifically for literature review automation.

### Phase 3: Advanced Statistics (10-15 min)
**Tools to test**: `run_mediation`, `run_moderation`, `run_pca`, `run_factor_analysis`, `calculate_sample_size`, `calculate_power`, `calculate_mde`

**Why critical**: Differentiates from basic calculator apps. PhD students need these for dissertations.

### Phase 4: Web Search & R Execution (5-10 min)
**Tools to test**: `web_search`, `run_r_code`

**Why critical**: Web search is the fallback when Archive API is down. R execution is promised feature.

### Phase 5: Cross-Domain Workflows (15-20 min)
**Test combinations**: Qualitative â†’ Stats, Literature â†’ Analysis, Multi-tool workflows

**Why critical**: Real users don't use single toolsâ€”they chain them together. Workflow engine must handle complex sequences.

---

## Detailed Test Cases

### PHASE 1: Qualitative Analysis Tools

#### Test 1.1: Load and Code Transcript
```bash
# Create sample transcript
cd ~/Downloads/data
cat > interview_transcript.txt << 'EOF'
Interviewer: Can you tell me about your experience with remote work?

Participant 1: I find remote work very isolating. I miss the casual conversations with colleagues at the office.

Participant 2: For me, remote work has been liberating. I can focus without constant interruptions and have more time with my family.

Participant 1: But the lack of clear boundaries between work and home life is really stressful. I'm working longer hours now.

Participant 3: I agree about the boundaries issue. However, I've learned to set strict schedules, and it's improved my work-life balance significantly.

Interviewer: How do you handle team collaboration remotely?

Participant 2: Video calls help, but they're exhausting. I prefer async communication like emails or Slack.

Participant 3: I think the key is having the right tools and establishing clear communication norms with your team.
EOF

# Test load and extract themes
cite-agent "Load the transcript from interview_transcript.txt and extract the main themes"
```

**Expected behavior**:
- âœ… Successfully loads transcript file
- âœ… Identifies themes (e.g., "isolation", "work-life boundaries", "communication challenges", "productivity")
- âœ… Clean formatting (no LaTeX, no stray backticks)
- âœ… Actionable output for researcher

**Red flags**:
- âŒ File not found errors
- âŒ Transcript not processed
- âŒ No themes extracted
- âŒ Workflow fails to complete

---

#### Test 1.2: Create Codes and Apply to Segments
```bash
cite-agent "Create a qualitative code called 'work-life-balance' with description 'Statements about boundaries between work and personal life'. Then code the segments in interview_transcript.txt that match this theme."
```

**Expected behavior**:
- âœ… Creates code with label and description
- âœ… Applies code to relevant text segments
- âœ… Shows which segments were coded
- âœ… Maintains context between steps (workflow sequencing)

**Red flags**:
- âŒ Code creation fails
- âŒ Segments not identified
- âŒ Context lost between create_code and code_segment steps
- âŒ Workflow engine doesn't sequence properly

---

#### Test 1.3: List Codes and Generate Codebook
```bash
cite-agent "List all qualitative codes created so far, then generate a comprehensive codebook summarizing the coding scheme"
```

**Expected behavior**:
- âœ… Lists all codes with descriptions
- âœ… Generates structured codebook
- âœ… Professional formatting for academic use
- âœ… Workflow handles multi-step process

**Red flags**:
- âŒ No codes found (state management issue)
- âŒ Codebook not generated
- âŒ Unprofessional output formatting

---

### PHASE 2: Literature Synthesis Tools

#### Test 2.1: Find Related Papers
```bash
cite-agent "Search for papers related to 'Attention Is All You Need' and show me 3 related papers"
```

**Expected behavior**:
- âœ… Uses Archive API to find related papers
- âœ… Returns relevant transformer/attention papers
- âœ… Shows titles, authors, years
- âœ… Clean formatting

**Red flags**:
- âŒ API connection fails
- âŒ No papers returned
- âŒ Irrelevant results
- âŒ Tool not recognized

---

#### Test 2.2: Build Paper Library and Synthesize
```bash
cite-agent "Search for 5 papers about 'neural machine translation', add them to my library, then synthesize them into a literature review covering key methods and findings"
```

**Expected behavior**:
- âœ… Searches papers (search_papers)
- âœ… Adds to library (add_paper Ã— 5)
- âœ… Synthesizes into coherent review (synthesize_literature)
- âœ… Professional academic writing
- âœ… Workflow sequences correctly: search â†’ add â†’ synthesize

**Red flags**:
- âŒ Papers not added to library
- âŒ Synthesis doesn't use library papers
- âŒ Context lost between steps
- âŒ Output is just concatenated abstracts (not synthesis)
- âŒ Workflow doesn't sequence properly

---

#### Test 2.3: Extract Themes and Find Gaps
```bash
cite-agent "From the papers in my library about neural machine translation, extract the main research themes and identify research gaps that haven't been addressed"
```

**Expected behavior**:
- âœ… Accesses library from previous test (state persistence)
- âœ… Extracts themes (extract_lit_themes)
- âœ… Identifies research gaps (find_research_gaps)
- âœ… Actionable insights for researcher
- âœ… Workflow handles multi-step analysis

**Red flags**:
- âŒ Library empty (state lost)
- âŒ Themes not extracted
- âŒ Gaps not identified
- âŒ Generic output (not specific to papers)

---

#### Test 2.4: Export to Zotero
```bash
cite-agent "Export my paper library to Zotero format and save it to a file"
```

**Expected behavior**:
- âœ… Exports library in valid Zotero format
- âœ… File created successfully
- âœ… All papers included with metadata
- âœ… Ready for import to Zotero

**Red flags**:
- âŒ Export fails
- âŒ Invalid format
- âŒ Missing papers
- âŒ File not created

---

### PHASE 3: Advanced Statistics Tools

#### Test 3.1: Mediation Analysis
```bash
# Create mediation test data
cd ~/Downloads/data
cat > mediation_data.csv << 'EOF'
X,M,Y
1,2,3
2,3,5
3,5,7
4,6,9
5,8,11
6,9,13
7,11,15
8,12,17
9,14,19
10,15,21
EOF

cite-agent "Run a mediation analysis with mediation_data.csv where X is the predictor, M is the mediator, and Y is the outcome. Show me the direct, indirect, and total effects."
```

**Expected behavior**:
- âœ… Loads CSV data
- âœ… Runs mediation analysis (statsmodels or similar)
- âœ… Reports direct effect, indirect effect, total effect
- âœ… Significance tests included
- âœ… Clean number formatting (no excessive decimals)

**Red flags**:
- âŒ Tool not recognized
- âŒ Analysis fails
- âŒ Results incomplete (missing effects)
- âŒ Formatting issues (LaTeX notation, excessive decimals)

---

#### Test 3.2: PCA (Principal Component Analysis)
```bash
# Create PCA test data
cat > pca_data.csv << 'EOF'
var1,var2,var3,var4
2.5,2.4,3.5,3.7
0.5,0.7,1.2,1.5
2.2,2.9,3.1,3.6
1.9,2.2,2.8,3.1
3.1,3.0,4.0,4.2
2.3,2.7,3.3,3.5
2.0,1.6,2.5,2.8
1.0,1.1,1.8,2.1
1.5,1.6,2.2,2.4
1.1,0.9,1.5,1.8
EOF

cite-agent "Run PCA on pca_data.csv and show me the explained variance by each component and the loadings"
```

**Expected behavior**:
- âœ… Runs PCA successfully
- âœ… Shows explained variance percentages
- âœ… Shows component loadings
- âœ… Clean formatting with proper decimals

**Red flags**:
- âŒ Analysis fails
- âŒ Results incomplete
- âŒ Formatting poor

---

#### Test 3.3: Power Analysis
```bash
cite-agent "Calculate the required sample size for a study with expected effect size of 0.5, power of 0.8, and alpha of 0.05 for a two-sample t-test"
```

**Expected behavior**:
- âœ… Runs power analysis (statsmodels.stats.power or similar)
- âœ… Returns required sample size
- âœ… Shows calculation parameters
- âœ… Actionable for researcher

**Red flags**:
- âŒ Tool not found
- âŒ Calculation incorrect
- âŒ No sample size returned

---

### PHASE 4: Web Search & R Execution

#### Test 4.1: Web Search
```bash
cite-agent "Search the web for latest AI breakthroughs in 2024 and summarize the top 3 findings"
```

**Expected behavior**:
- âœ… Performs web search
- âœ… Returns relevant recent results
- âœ… Summarizes findings
- âœ… Clean formatting

**Red flags**:
- âŒ Tool fails
- âŒ No results returned
- âŒ Results irrelevant or outdated

---

#### Test 4.2: R Code Execution
```bash
cite-agent "Write R code to calculate the mean and standard deviation of the vector c(10, 20, 30, 40, 50) and run it to show me the results"
```

**Expected behavior**:
- âœ… Generates R code
- âœ… Executes R code successfully
- âœ… Shows output (mean: 30, sd: 15.81)
- âœ… Clean formatting

**Red flags**:
- âŒ R not available
- âŒ Code doesn't execute
- âŒ Execution errors
- âŒ No output returned

---

### PHASE 5: Cross-Domain Workflow Combinations

#### Test 5.1: Qualitative â†’ Quantitative Pipeline
```bash
cite-agent "Load interview_transcript.txt, extract themes, then create a frequency count of how many times each theme appears across all participants. Visualize this as a bar chart concept (describe the chart)."
```

**Expected behavior**:
- âœ… Loads transcript (load_transcript)
- âœ… Extracts themes (extract_themes)
- âœ… Counts frequencies (run_python_code or analyze_data)
- âœ… Describes visualization (plot_data)
- âœ… Workflow sequences: qualitative â†’ analysis â†’ visualization
- âœ… Context maintained throughout

**Red flags**:
- âŒ Context lost between qualitative and quantitative steps
- âŒ Theme data not passed to analysis step
- âŒ Workflow breaks at tool boundaries
- âŒ No visualization description

---

#### Test 5.2: Literature â†’ Data â†’ Analysis Pipeline
```bash
cite-agent "Search for papers about 'student performance prediction', find the most cited one, then create a mock dataset with 50 students having variables (study_hours, attendance, gpa) and run a regression to predict GPA from study hours and attendance"
```

**Expected behavior**:
- âœ… Searches papers (search_papers)
- âœ… Identifies most cited
- âœ… Generates mock data (run_python_code or create dataset)
- âœ… Runs regression (run_regression)
- âœ… Reports RÂ², coefficients, p-values
- âœ… Complex workflow sequences correctly: research â†’ data generation â†’ analysis
- âœ… Context flows through entire pipeline

**Red flags**:
- âŒ Steps disconnected (agent treats as separate queries)
- âŒ Data not generated
- âŒ Regression doesn't use generated data
- âŒ Workflow engine fails on complex sequences
- âŒ Context window limitations cause information loss

---

#### Test 5.3: Multi-Domain Research Workflow
```bash
cite-agent "I'm researching remote work impact on productivity. First, search for 3 papers on this topic. Then, create a qualitative code for 'productivity factors' and apply it to interview_transcript.txt. Finally, search web for latest 2024 statistics on remote work productivity and synthesize all findings into a short research brief."
```

**Expected behavior**:
- âœ… Literature search (search_papers)
- âœ… Qualitative coding (create_code, code_segment)
- âœ… Web search (web_search) for current data
- âœ… Synthesis across all sources
- âœ… Coherent research brief output
- âœ… **ULTIMATE WORKFLOW TEST**: sequences across all major tool categories
- âœ… Context maintained across 5+ tool invocations

**Red flags**:
- âŒ Agent treats as separate queries (no workflow sequencing)
- âŒ Steps don't connect (e.g., synthesis ignores qualitative coding)
- âŒ Context lost partway through
- âŒ Workflow engine can't handle this complexity
- âŒ Output is disjointed (not synthesized)

---

## Success Criteria

### Minimum Acceptable (Ship Blocker if Not Met)
- âœ… **Qualitative tools**: At least 3/5 tools working (load_transcript, create_code, extract_themes)
- âœ… **Literature tools**: At least 4/7 untested tools working (find_related, add_paper, synthesize, find_gaps)
- âœ… **Workflow sequencing**: Cross-domain workflows complete successfully
- âœ… **No regressions**: Previously working tools still work
- âœ… **Output quality**: Clean formatting (no LaTeX, no stray backticks, smart number formatting)

### Ideal Outcome (High Confidence Ship)
- âœ… **All qualitative tools working** (5/5)
- âœ… **All literature tools working** (7/7 untested)
- âœ… **Advanced stats**: At least 3/7 working (mediation, PCA, power)
- âœ… **Web search**: Working as fallback
- âœ… **R execution**: Working (Python already verified)
- âœ… **Complex workflows**: Multi-domain sequences work flawlessly

### Ship-With-Documentation (Acceptable)
- âœ… Core qualitative + literature + workflow sequencing works
- ðŸ“ Document which advanced stats tools are untested
- ðŸ“ Note R execution may need additional testing
- ðŸ“ Known limitations clearly stated in README

---

## Risk Assessment

### High Risk (Must Test)
1. **Qualitative analysis suite** - Unique differentiator, no fallback
2. **Literature synthesis workflows** - Core academic use case
3. **Cross-domain sequencing** - Real users chain tools together

### Medium Risk (Should Test)
4. **Advanced statistics** - PhD students need these, but basic stats verified
5. **Web search** - Fallback mechanism, but Archive API is primary

### Low Risk (Can Document)
6. **R execution** - Python works, R likely similar
7. **Individual advanced stats** - Can document untested tools

---

## Testing Timeline

**Estimated total**: 60-90 minutes

| Phase | Time | Priority |
|-------|------|----------|
| Phase 1: Qualitative | 15-20 min | ðŸ”´ CRITICAL |
| Phase 2: Literature | 15-20 min | ðŸ”´ CRITICAL |
| Phase 3: Advanced Stats | 10-15 min | ðŸŸ¡ HIGH |
| Phase 4: Web + R | 5-10 min | ðŸŸ¡ MEDIUM |
| Phase 5: Cross-Domain | 15-20 min | ðŸ”´ CRITICAL |

**CRITICAL PATH**: Phases 1, 2, 5 (45-60 min)  
**FULL COVERAGE**: All phases (60-90 min)

---

## Post-Testing Actions

### If All Tests Pass
1. âœ… Update test coverage report (28% â†’ 70%+)
2. âœ… Proceed to Windows testing
3. âœ… Ship v1.5.7 to PyPI with confidence
4. âœ… Update README with verified capabilities

### If Issues Found
1. ðŸ”§ Document specific failures
2. ðŸ”§ Assess severity (ship-blocker vs. known limitation)
3. ðŸ”§ Fix critical issues or document limitations
4. ðŸ”§ Re-test affected functionality
5. ðŸ”§ Update KNOWN_ISSUES.md

### If Major Failures
1. âš ï¸ Delay ship
2. âš ï¸ Debug and fix broken functionality
3. âš ï¸ Full regression testing
4. âš ï¸ Re-assess v1.5.7 readiness

---

## Notes

- **Why these tests matter**: These are the features that differentiate cite-agent from generic AI assistants
- **Why now**: v1.5.7 includes critical formatting fixes; want to ship with confidence across all major features
- **Why comprehensive**: Real users don't use tools in isolationâ€”they build complex workflows
- **Time investment**: 60-90 minutes of testing prevents hours of user-reported bugs and reputation damage

**Bottom line**: If qualitative analysis or literature synthesis is broken, cite-agent loses its core value proposition. Better to find out now than after PyPI ship.
