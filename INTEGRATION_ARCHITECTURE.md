# Integration Architecture - Enterprise Infrastructure Modules

**Document Version:** 1.0
**Date:** 2025-11-05
**Status:** Implementation Ready

---

## ğŸ¯ Overview

This document describes how 6 enterprise-grade infrastructure modules integrate into the EnhancedNocturnalAgent to create a production-ready, fault-tolerant, self-healing system.

**Modules:**
1. Circuit Breaker (370 lines) - Failure detection & fast-fail
2. Request Queue (390 lines) - Backpressure & prioritization
3. Observability (398 lines) - Metrics & tracing
4. Self-Healing (418 lines) - Auto-recovery
5. Adaptive Providers (413 lines) - Intelligent provider selection
6. Execution Safety (329 lines) - Command validation

**Total Infrastructure:** 2,318 lines of production-grade code

---

## ğŸ—ï¸ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EnhancedNocturnalAgent                       â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    process_request()                    â”‚   â”‚
â”‚  â”‚  (Main entry point - wraps with queue & metrics)       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚            IntelligentRequestQueue                      â”‚   â”‚
â”‚  â”‚  - Priority: URGENT > NORMAL > BATCH                   â”‚   â”‚
â”‚  â”‚  - Per-user concurrency limits (3 concurrent)          â”‚   â”‚
â”‚  â”‚  - Global concurrency limit (50 concurrent)            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚            ObservabilityManager                         â”‚   â”‚
â”‚  â”‚  - Request tracing (start/end)                         â”‚   â”‚
â”‚  â”‚  - Metrics: requests_total, requests_success/error     â”‚   â”‚
â”‚  â”‚  - Latency histograms (p50, p95, p99)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         _process_request_impl() [CORE LOGIC]           â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚  Request Analysis & Routing                      â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - Classify query type (academic/financial/etc)  â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - Determine if local-only or needs backend      â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚  Shell Operations (if local)                     â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - Execute commands via ExecutionSafetyLayer     â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - Validate commands, block dangerous patterns   â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - Audit log all executions                      â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚  Backend API Calls (if backend needed)           â”‚ â”‚   â”‚
â”‚  â”‚  â”‚                                                   â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  Step 1: Provider Selection                      â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - AdaptiveProviderRouter.select_provider()      â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - Based on query type & historical performance  â”‚ â”‚   â”‚
â”‚  â”‚  â”‚                                                   â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  Step 2: Circuit Breaker Check                   â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - CircuitBreaker.is_open() â†’ fast-fail if open  â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - Prevents cascading failures                   â”‚ â”‚   â”‚
â”‚  â”‚  â”‚                                                   â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  Step 3: Call with Recovery                      â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - CircuitBreaker.call()                         â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - SelfHealingWrapper.execute_with_recovery()    â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - Auto-retry with exponential backoff           â”‚ â”‚   â”‚
â”‚  â”‚  â”‚                                                   â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  Step 4: Record Result                           â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - AdaptiveProviderRouter.record_result()        â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - Learn from success/failure                    â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  - Update provider performance metrics           â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Module Integration Points

### 1. Circuit Breaker Integration

**File:** `cite_agent/circuit_breaker.py`
**Integration Location:** `enhanced_ai_agent.py` - wraps `call_backend_query()`

**Initialization (in `__init__()`):**
```python
from .circuit_breaker import CircuitBreaker, CircuitBreakerConfig, CircuitState

self.backend_circuit = CircuitBreaker(
    name="backend_api",
    config=CircuitBreakerConfig(
        failure_threshold=0.5,      # Open if >50% requests fail
        min_requests_for_decision=10,
        open_timeout=30.0,          # Try recovery after 30s
        half_open_max_calls=3
    )
)
```

**Usage (in `call_backend_query()`):**
```python
async def call_backend_query(self, query: str, history: List, api_results: Dict):
    # Check circuit breaker first (fast-fail)
    if self.backend_circuit.is_open():
        self.metrics.increment("backend_circuit_open")
        return ChatResponse(
            response="Backend temporarily unavailable (auto-recovering). Using local mode.",
            error_message="Circuit breaker open"
        )

    # Wrap call with circuit breaker
    try:
        result = await self.backend_circuit.call(
            self._do_backend_query_impl,
            query,
            history,
            api_results
        )
        return result
    except CircuitBreakerOpenError:
        # Circuit opened during call - fail fast
        return ChatResponse(
            response="Backend unavailable. Switched to local mode.",
            error_message="Circuit breaker opened"
        )
```

**Benefits:**
- Fast-fail when backend is down (< 1s instead of 30s timeout)
- Automatic recovery detection
- Prevents cascading failures

---

### 2. Request Queue Integration

**File:** `cite_agent/request_queue.py`
**Integration Location:** `enhanced_ai_agent.py` - wraps `process_request()`

**Initialization (module-level singleton):**
```python
from .request_queue import IntelligentRequestQueue, RequestPriority

# Global queue singleton
_request_queue = None

def get_request_queue() -> IntelligentRequestQueue:
    global _request_queue
    if _request_queue is None:
        _request_queue = IntelligentRequestQueue(
            max_concurrent_global=50,
            max_concurrent_per_user=3,
            queue_size_limit=1000
        )
    return _request_queue
```

**Usage (in `process_request()`):**
```python
async def process_request(
    self,
    request: ChatRequest,
    priority: RequestPriority = RequestPriority.NORMAL
) -> ChatResponse:
    queue = get_request_queue()

    # Check if we should queue (based on load)
    if queue.should_queue():
        # Submit to queue with priority
        result = await queue.submit(
            self._process_request_impl,
            request,
            priority=priority,
            user_id=request.user_id or "anonymous"
        )
        return result
    else:
        # Direct execution if not under load
        return await self._process_request_impl(request)
```

**Benefits:**
- Prevents thundering herd (max 50 concurrent globally)
- Per-user fairness (max 3 concurrent per user)
- Priority handling (urgent requests jump queue)
- Graceful degradation under load

---

### 3. Observability Integration

**File:** `cite_agent/observability.py`
**Integration Location:** Throughout `enhanced_ai_agent.py`

**Initialization (in `__init__()`):**
```python
from .observability import ObservabilityManager, EventType

self.metrics = ObservabilityManager(
    service_name="cite-agent",
    enable_metrics=True,
    enable_tracing=True
)
```

**Usage (instrument all major operations):**

**In `process_request()`:**
```python
async def process_request(self, request: ChatRequest) -> ChatResponse:
    # Start trace
    with self.metrics.trace_request(request.conversation_id):
        self.metrics.increment("requests_total")
        start_time = time.time()

        try:
            result = await self._process_request_impl(request)

            # Record success
            duration_ms = (time.time() - start_time) * 1000
            self.metrics.record_latency("request_duration_ms", duration_ms)
            self.metrics.increment("requests_success")

            return result
        except Exception as e:
            self.metrics.increment("requests_error")
            self.metrics.record_event(
                EventType.REQUEST_FAILED,
                user_id=request.user_id,
                error_message=str(e)
            )
            raise
```

**In `call_backend_query()`:**
```python
async def call_backend_query(self, query: str, history: List, api_results: Dict):
    self.metrics.increment("backend_calls_total")
    start = time.time()

    try:
        result = await self._do_backend_call(query, history, api_results)

        duration_ms = (time.time() - start) * 1000
        self.metrics.record_latency("backend_latency_ms", duration_ms)
        self.metrics.increment("backend_calls_success")

        return result
    except Exception as e:
        self.metrics.increment("backend_calls_error")
        raise
```

**Benefits:**
- Real-time metrics dashboard
- Performance tracking (p50, p95, p99)
- Failure analysis
- Capacity planning data

---

### 4. Self-Healing Integration

**File:** `cite_agent/self_healing.py`
**Integration Location:** `enhanced_ai_agent.py` - wraps error-prone operations

**Initialization (in `__init__()`):**
```python
from .self_healing import SelfHealingAgent, FailureType, RecoveryAction

self.self_healing = SelfHealingAgent()

# Register recovery callbacks
self.self_healing.register_callback(
    RecoveryAction.SWITCH_PROVIDER,
    self._switch_to_fallback_provider
)
self.self_healing.register_callback(
    RecoveryAction.DEGRADE_MODE,
    self._enable_degraded_mode
)
```

**Usage (wrap shell execution):**
```python
async def execute_command(self, command: str) -> str:
    """Execute command with self-healing recovery"""

    async def _execute_impl():
        return await self._execute_command_raw(command)

    # Wrap with self-healing
    result = await self.self_healing.execute_with_recovery(
        _execute_impl,
        fallback=lambda: "Command failed (attempting recovery)",
        max_retries=3,
        backoff_multiplier=2.0,
        recovery_actions=[RecoveryAction.RETRY_EXPONENTIAL]
    )

    return result
```

**Usage (wrap backend calls):**
```python
async def call_backend_query(self, query: str, history: List, api_results: Dict):
    """Backend call with self-healing"""

    async def _call_impl():
        return await self._do_backend_call(query, history, api_results)

    # Detect if provider is slow
    if self.self_healing.detect_slow_provider(
        provider=self.current_provider,
        latency_ms=self.last_call_latency,
        recent_latencies=self.recent_latencies
    ):
        # Auto-switch to faster provider
        await self.self_healing.recover(
            FailureType.PROVIDER_SLOW,
            RecoveryAction.SWITCH_PROVIDER
        )

    result = await self.self_healing.execute_with_recovery(_call_impl)
    return result
```

**Benefits:**
- Automatic error recovery
- Learns what recovery works
- Reduces manual intervention
- Improves reliability

---

### 5. Adaptive Providers Integration

**File:** `cite_agent/adaptive_providers.py`
**Integration Location:** `enhanced_ai_agent.py` - before LLM calls

**Initialization (in `__init__()`):**
```python
from .adaptive_providers import AdaptiveProviderRouter, QueryType

self.provider_router = AdaptiveProviderRouter(
    providers=["cerebras", "groq", "openai"],
    learning_rate=0.1
)
```

**Usage (intelligent provider selection):**
```python
async def call_backend_query(self, query: str, history: List, api_results: Dict):
    # Classify query type
    query_type = self._classify_query_type(query)

    # Select best provider based on historical performance
    provider = self.provider_router.select_provider(
        query_type=query_type,
        fallback="groq"
    )

    start_time = time.time()

    try:
        # Make call with selected provider
        result = await self._call_with_provider(provider, query, history, api_results)

        # Record successful result
        latency_ms = (time.time() - start_time) * 1000
        cost = self._estimate_cost(provider, result)

        self.provider_router.record_result(
            provider=provider,
            query_type=query_type,
            success=True,
            latency_ms=latency_ms,
            cost=cost
        )

        return result
    except Exception as e:
        # Record failure
        self.provider_router.record_result(
            provider=provider,
            query_type=query_type,
            success=False,
            latency_ms=(time.time() - start_time) * 1000
        )
        raise

def _classify_query_type(self, query: str) -> QueryType:
    """Classify query to select best provider"""
    query_lower = query.lower()

    if any(w in query_lower for w in ["paper", "citation", "doi", "research"]):
        return QueryType.ACADEMIC_PAPER
    elif any(w in query_lower for w in ["stock", "price", "ticker", "financial"]):
        return QueryType.FINANCIAL_DATA
    elif any(w in query_lower for w in ["code", "python", "function", "debug"]):
        return QueryType.CODE_GENERATION
    elif any(w in query_lower for w in ["analyze", "data", "csv", "statistics"]):
        return QueryType.DATA_ANALYSIS
    elif any(w in query_lower for w in ["search", "find", "google", "web"]):
        return QueryType.WEB_SEARCH
    else:
        return QueryType.CONVERSATION
```

**Benefits:**
- Learns best provider per query type
- Optimizes for speed + cost + accuracy
- Automatic provider switching
- Performance improves over time

---

### 6. Execution Safety Integration

**File:** `cite_agent/execution_safety.py`
**Integration Location:** `enhanced_ai_agent.py` - before shell execution

**Initialization (in `__init__()`):**
```python
from .execution_safety import (
    CommandExecutionValidator,
    CommandPlan,
    CommandClassification,
    CommandAuditLevel
)

self.safety = CommandExecutionValidator(
    audit_level=CommandAuditLevel.STRICT
)
```

**Usage (validate before execution):**
```python
async def execute_command(self, command: str, user_id: str = None) -> str:
    """Execute command with safety validation"""

    # Classify command
    classification = self.safety.classify_command(command)

    # Block dangerous commands
    if classification == CommandClassification.BLOCKED:
        self.safety.log_blocked_command(command, user_id)
        return f"â›” Command blocked for safety: {command}"

    # Warn on dangerous commands
    if classification == CommandClassification.DANGEROUS:
        self.safety.log_dangerous_command(command, user_id)
        return f"âš ï¸ Dangerous command requires user confirmation: {command}"

    # Create plan
    plan = CommandPlan(
        command=command,
        classification=classification,
        reason="User requested shell operation",
        max_execution_time_s=30.0
    )

    # Execute with validation
    try:
        output = await self._execute_command_raw(command)

        # Record successful execution
        execution = self.safety.create_execution_record(
            plan=plan,
            output=output,
            exit_code=0,
            execution_time_s=time.time() - start_time,
            user_id=user_id
        )

        self.safety.audit_log.append(execution)
        return output
    except Exception as e:
        # Record failed execution
        execution = self.safety.create_execution_record(
            plan=plan,
            output="",
            exit_code=1,
            error=str(e),
            user_id=user_id
        )

        self.safety.audit_log.append(execution)
        raise
```

**Benefits:**
- Prevents dangerous command execution
- Audit trail for compliance
- Command injection prevention
- User safety

---

## ğŸ”„ Complete Request Flow

Here's how a complete request flows through all modules:

### Example: User asks "Show me Python files in current directory"

**Step 1: Entry Point**
```python
process_request(request)
  â”‚
  â”œâ”€â–º RequestQueue.should_queue() â†’ No (not under load)
  â””â”€â–º _process_request_impl(request)
```

**Step 2: Observability Tracking**
```python
ObservabilityManager.trace_request("conv-123")
  â”œâ”€â–º metrics.increment("requests_total")
  â””â”€â–º Start timer for latency tracking
```

**Step 3: Request Analysis**
```python
_analyze_request_type(request)
  â”œâ”€â–º Detect: File listing operation
  â”œâ”€â–º Classification: LOCAL (no backend needed)
  â””â”€â–º Route to: Shell execution
```

**Step 4: Safety Validation**
```python
ExecutionSafetyLayer.classify_command("find . -name '*.py'")
  â”œâ”€â–º Classification: SAFE (read-only)
  â”œâ”€â–º Create plan with hash
  â””â”€â–º Approve execution
```

**Step 5: Self-Healing Execution**
```python
SelfHealingAgent.execute_with_recovery(execute_command)
  â”œâ”€â–º Attempt 1: execute_command("find . -name '*.py'")
  â”œâ”€â–º Success! âœ“
  â””â”€â–º Return output
```

**Step 6: Metrics Recording**
```python
ObservabilityManager.record_latency("request_duration_ms", 125)
  â”œâ”€â–º metrics.increment("requests_success")
  â”œâ”€â–º metrics.increment("shell_operations_total")
  â””â”€â–º End trace
```

**Step 7: Response**
```python
Return ChatResponse(
    response="Found 12 Python files:\n- enhanced_ai_agent.py\n- config.py\n...",
    tools_used=["shell_execution"],
    metadata={"duration_ms": 125, "files_found": 12}
)
```

### Example: User asks "Find papers about quantum computing"

**Step 1-2:** Same (queue check, observability)

**Step 3: Request Analysis**
```python
_analyze_request_type(request)
  â”œâ”€â–º Detect: Academic paper search
  â”œâ”€â–º Classification: BACKEND_REQUIRED
  â””â”€â–º Route to: call_backend_query()
```

**Step 4: Provider Selection**
```python
AdaptiveProviderRouter.select_provider(QueryType.ACADEMIC_PAPER)
  â”œâ”€â–º Check historical performance
  â”œâ”€â–º Cerebras: 95% success, 1.2s avg, score=93.8
  â”œâ”€â–º Groq: 90% success, 0.8s avg, score=89.2
  â””â”€â–º Selected: Cerebras (best for academic queries)
```

**Step 5: Circuit Breaker Check**
```python
CircuitBreaker.is_open() â†’ False (CLOSED)
  â”œâ”€â–º State: CLOSED (healthy)
  â”œâ”€â–º Failure rate: 5% (below threshold)
  â””â”€â–º Proceed with call
```

**Step 6: Backend Call with Recovery**
```python
CircuitBreaker.call(call_backend_query)
  â””â”€â–º SelfHealingAgent.execute_with_recovery()
      â”œâ”€â–º Attempt 1: Call backend with Cerebras
      â”œâ”€â–º Latency: 1.35s
      â”œâ”€â–º Success! âœ“
      â””â”€â–º Return papers data
```

**Step 7: Record Result & Learn**
```python
AdaptiveProviderRouter.record_result(
    provider="cerebras",
    query_type=QueryType.ACADEMIC_PAPER,
    success=True,
    latency_ms=1350,
    cost=0.0012
)
  â””â”€â–º Update Cerebras performance profile
      â”œâ”€â–º Total requests: 156 â†’ 157
      â”œâ”€â–º Avg latency: 1.20s â†’ 1.21s
      â””â”€â–º Score: 93.8 â†’ 93.7
```

**Step 8:** Metrics recording and response (same as Example 1)

---

## ğŸ¯ Integration Success Criteria

### Functional Requirements
- âœ… All 6 modules imported and initialized
- âœ… Circuit breaker wraps backend calls
- âœ… Request queue manages concurrency
- âœ… Observability tracks all operations
- âœ… Self-healing recovers from errors
- âœ… Adaptive routing selects best provider
- âœ… Execution safety validates commands

### Performance Requirements
- âœ… Circuit breaker fails fast (< 1s)
- âœ… Queue prevents overload (max 50 concurrent)
- âœ… Metrics add < 5ms overhead per request
- âœ… Self-healing recovers 95%+ of failures
- âœ… Adaptive routing improves over time

### Testing Requirements
- âœ… Unit tests for each module
- âœ… Integration tests for module interactions
- âœ… Real conversation tests
- âœ… Stress tests (100 concurrent users)

---

## ğŸ“ File Structure After Integration

```
cite_agent/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ enhanced_ai_agent.py         # MAIN - imports all modules
â”‚   â”œâ”€â”€ [NEW] import circuit_breaker
â”‚   â”œâ”€â”€ [NEW] import request_queue
â”‚   â”œâ”€â”€ [NEW] import observability
â”‚   â”œâ”€â”€ [NEW] import self_healing
â”‚   â”œâ”€â”€ [NEW] import adaptive_providers
â”‚   â””â”€â”€ [NEW] import execution_safety
â”‚
â”œâ”€â”€ circuit_breaker.py            # Circuit breaker pattern
â”œâ”€â”€ request_queue.py              # Priority queue with backpressure
â”œâ”€â”€ observability.py              # Metrics & tracing
â”œâ”€â”€ self_healing.py               # Auto-recovery
â”œâ”€â”€ adaptive_providers.py         # Intelligent provider selection
â”œâ”€â”€ execution_safety.py           # Command validation
â””â”€â”€ config.py                     # Configuration management
```

---

## ğŸš€ Implementation Plan

### Phase 1: Foundation (1-2 hours) âœ“
- âœ… Create this architecture document
- [ ] Import all modules into enhanced_ai_agent.py
- [ ] Initialize all modules in __init__()
- [ ] Update config.py with module settings

### Phase 2: Core Infrastructure (3-4 hours)
- [ ] Integrate Circuit Breaker around call_backend_query()
- [ ] Integrate Request Queue around process_request()
- [ ] Integrate Observability throughout agent

### Phase 3: Intelligence Layer (2-3 hours)
- [ ] Integrate Self-Healing error recovery
- [ ] Integrate Adaptive Provider selection
- [ ] Integrate Execution Safety validation

### Phase 4: Bug Fixes (2-3 hours)
- [ ] Fix _is_location_query() intent detection
- [ ] Add local-only mode for shell operations
- [ ] Improve request routing logic

### Phase 5: Testing (2-3 hours)
- [ ] Module integration tests
- [ ] Real conversation tests
- [ ] Stress tests

### Phase 6: Documentation (1-2 hours)
- [ ] Update all documentation
- [ ] Performance tuning
- [ ] Final commit and push

---

## ğŸ“ Notes

**Current Status:** Architecture defined, ready for implementation

**Estimated Total Time:** 12-15 hours
**Estimated Credits:** 100-150 credits

**Next Step:** Import all modules into enhanced_ai_agent.py and begin initialization

---

*This will be a true enterprise system when we're done.*
